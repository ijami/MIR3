{"id":220151284,"name":"Context-Aware Bandits","abstraction":"Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.","authors":["Jean-Yves Audibert"],"citedInUrls":["https://www.researchgate.net/publication/282844171_Context-Aware_Bandits","https://www.researchgate.net/publication/282310775_Algorithms_for_Linear_Bandits_on_Polyhedral_Sets","https://www.researchgate.net/publication/281670472_Asymptotically_Optimal_Multi-Armed_Bandit_Policies_under_a_Cost_Constraint"],"refrenceUrls":["https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","https://www.researchgate.net/publication/38362521_On_Tail_Probabilities_for_Martingales","https://www.researchgate.net/publication/201976635_On_the_Likelihood_That_One_Unknown_Probability_Exceeds_Another_in_View_of_the_Evidence_of_Two_Samples","https://www.researchgate.net/publication/247441127_Sample_Mean_Based_Index_Policies_with_Olog_n_Regret_for_the_Multi-Armed_Bandit_Problem","https://www.researchgate.net/publication/221497328_Improved_Rates_for_the_Stochastic_Continuum-Armed_Bandit_Problem","https://www.researchgate.net/publication/221996154_Probability_Inequalities_For_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/228057769_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/3022289_Machine_Learning_and_Nonparametric_Bandit_Theory","https://www.researchgate.net/publication/221112399_Bandit_Based_Monte-Carlo_Planning","https://www.researchgate.net/publication/238378872_Modification_of_UCT_with_Patterns_in_Monte-Carlo_Go"],"citedInIDs":[282844171,282310775,281670472],"refrenceIDs":[220343796,38362521,201976635,247441127,221497328,221996154,228057769,3022289,221112399,238378872],"pageRank":0.0012744321583293252}
