{"id":282639101,"name":"Feature Extraction Techniques and Classification Algorithms for EEG Signals to detect Human Stress - A Review","abstraction":"This paper examines the role and efficiency of the non-convex loss functions for binary classification problems. In particular, we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data. The analysis of the role of a particular non-convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss -- the ability of the loss to efficiently adapt to the outlying data, the local convex properties of the loss and the proportion of the contaminated data. In order to use these properties efficiently, we propose a new family of non-convex losses named $\\gamma$-robust losses. Moreover, we present a new boosting framework, {\\it Arch Boost}, designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination. Along with the Arch Boosting framework, the non-convex losses lead to the new class of boosting algorithms, named adaptive, robust, boosting (ARB). Furthermore, we present theoretical examples that demonstrate the robustness properties of the proposed algorithms. In particular, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. Moreover, we present new theoretical results, based only on local curvatures, which may be used to establish statistical and optimization properties of the proposed Arch boosting algorithms with highly non-convex loss functions. Extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers.","authors":["Alexander Hanbo Li"],"citedInUrls":[],"refrenceUrls":["https://www.researchgate.net/publication/4742228_Convexity_Classification_and_Risk_Bounds","https://www.researchgate.net/publication/221619607_AdaBoost_is_Consistent","https://www.researchgate.net/publication/38349013_Population_theory_for_boosting_ensembles","https://www.researchgate.net/publication/277187455_Consistency_and_robustness_of_kernel_based_regression","https://www.researchgate.net/publication/2865867_Logistic_Regression_AdaBoost_and_Bregman_Distances","https://www.researchgate.net/publication/257756465_PLIDA_Cross-platform_gene_expression_normalization_using_perturbed_topic_models","https://www.researchgate.net/publication/2454925_An_Empirical_Comparison_of_Three_Methods_for_Constructing_Ensembles_of_Decision_Trees_Bagging_Boosting_and_Randomization_Machine_Learning_402139_-_157","https://www.researchgate.net/publication/246742647_Madaboost_a_modi_ed_version_of_adaboost","https://www.researchgate.net/publication/2568812_An_Adaptive_Version_of_the_Boost_By_Majority_Algorithm","https://www.researchgate.net/publication/201841001_A_Decision-Theoretic_Generalization_of_On-Line_Learning_and_an_Application_to_Boosting"],"citedInIDs":[],"refrenceIDs":[4742228,221619607,38349013,277187455,2865867,257756465,2454925,246742647,2568812,201841001],"pageRank":5.314734299140724E-4}
