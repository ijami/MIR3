{"id":274320154,"name":"Stochastic Mirror Descent Algorithm for L1-Regularized Risk Minimizations.","abstraction":"We consider the problem of learning convex aggregation of models, that is as good as the best convex aggregation, for the binary classification problem. Working in the stream based active learning setting, where the active learner has to make a decision on-the-fly, if it wants to query for the label of the point currently seen in the stream, we propose a stochastic-mirror descent algorithm, called SMD-AMA, with entropy regularization. We establish an excess risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of the order of $O\\left(\\sqrt{\\frac{\\log(M)}{{T^{1-\\mu}}}}\\right)$, where $\\mu\\in [0,1)$ is an algorithm dependent parameter, that trades-off the number of labels queried, and excess risk.","authors":["Ravi Ganti"],"citedInUrls":[],"refrenceUrls":["https://www.researchgate.net/publication/225540813_Lecture_Notes_in_Computer_Science","https://www.researchgate.net/publication/221653938_Active_learning_using_adaptive_resampling","https://www.researchgate.net/publication/2560520_Statistical_Behavior_and_Consistency_of_Classification_Methods_based_on_Convex_Risk_Minimization"],"citedInIDs":[],"refrenceIDs":[225540813,221653938,2560520],"pageRank":5.314734299140724E-4}
