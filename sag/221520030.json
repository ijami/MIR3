{"id":221520030,"name":"Strategies and Principles of Distributed Machine Learning on Big Data","abstraction":"Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets. In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.","authors":["Mohamed Aly","Joseph Gonzalez","Shravan Narayanamurthy"],"citedInUrls":["https://www.researchgate.net/publication/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","https://www.researchgate.net/publication/283334510_WarpLDA_a_Simple_and_Efficient_O1_Algorithm_for_Latent_Dirichlet_Allocation","https://www.researchgate.net/publication/283118017_High_Performance_Latent_Variable_Models"],"refrenceUrls":["https://www.researchgate.net/publication/221669680_Stochastic_Relaxation_Gibbs_Distributionsand_the_Bayesian_Restoration_of_Images","https://www.researchgate.net/publication/5948829_Finding_Scientific_Topics","https://www.researchgate.net/publication/221590687_Consistent_Hashing_and_Random_Trees_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web","https://www.researchgate.net/publication/220618451_Rackoff_C_How_to_construct_pseudorandom_permutations_from_pseudorandom_functions_SIAM_J_Comput_172_373-386","https://www.researchgate.net/publication/2251101_Criticality_and_Parallelism_in_Combinatorial_Optimization","https://www.researchgate.net/publication/221346419_Bayesian_haplo-type_inference_via_the_dirichlet_process","https://www.researchgate.net/publication/220320831_Parallel_Gibbs_Sampling_From_Colored_Fields_to_Thin_Junction_Trees","https://www.researchgate.net/publication/221619032_Asynchronous_Distributed_Learning_of_Topic_Models","https://www.researchgate.net/publication/2395793_Consistent_Hashing_and_Random_Trees_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web","https://www.researchgate.net/publication/220320734_Distributed_Algorithms_for_Topic_Models"],"citedInIDs":[288889800,283334510,283118017],"refrenceIDs":[221669680,5948829,221590687,220618451,2251101,221346419,220320831,221619032,2395793,220320734],"pageRank":8.100463616739775E-4}
