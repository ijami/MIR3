{"id":226869786,"name":"Adaptive Online Learning","abstraction":"We study model selection strategies based on penalized empirical loss minimization. We point out a tight relationship between error estimation and data-based complexity penalization: any good error estimate may be converted into a data-based penalty function and the performance of the estimate is governed by the quality of the error estimate. We consider several penalty functions, involving error estimates on independent test data, empirical VC dimension, empirical VC entropy, and margin-based quantities. We also consider the maximal difference between the error on the first half of the training data and the second half, and the expected maximal discrepancy, a closely related capacity estimate that can be calculated by Monte Carlo integration. Maximal discrepancy penalty functions are appealing for pattern classification problems, since their computation is equivalent to empirical risk minimization over the training data with some labels flipped.","authors":[],"citedInUrls":["https://www.researchgate.net/publication/281227646_Adaptive_Online_Learning","https://www.researchgate.net/publication/272844461_Generalization_Performance_of_Radial_Basis_Function_Networks","https://www.researchgate.net/publication/268794264_A_Deep_Connection_Between_the_Vapnik-Chervonenkis_Entropy_and_the_Rademacher_Complexity"],"refrenceUrls":["https://www.researchgate.net/publication/3027229_A_New_Look_At_The_Statistical_Model_Identification","https://www.researchgate.net/publication/238681227_Complexity_Regularization_with_Application_to_Artificial_Neural_Networks","https://www.researchgate.net/publication/220680103_Correction_to_%27Minimum_Complexity_Density_Estimation%27","https://www.researchgate.net/publication/3079437_Bartlett_PL_The_Sample_Complexity_of_Pattern_Classification_with_Neural_Networks_The_Size_of_the_Weights_is_More_Important_than_the_Size_of_the_Network_IEEE_Transactions_on_Information_Theory_442_525-","https://www.researchgate.net/publication/3022512_Learning_by_canonical_smooth_estimation_-_Part_I_Simultaneous_estimation","https://www.researchgate.net/publication/3022517_Learning_by_Canonical_Smooth_Estimation_Part_II_Learning_and_Choice_of_Model_Complexity","https://www.researchgate.net/publication/2662122_Self_Bounding_Learning_Algorithms","https://www.researchgate.net/publication/38358827_Nonparametric_Maximum_Likelihood_Estimation_by_the_Method_of_Sieves","https://www.researchgate.net/publication/38361515_Some_Limit_Theorems_for_Empirical_Processes","https://www.researchgate.net/publication/221996154_Probability_Inequalities_For_Sums_of_Bounded_Random_Variables"],"citedInIDs":[281227646,272844461,268794264],"refrenceIDs":[3027229,238681227,220680103,3079437,3022512,3022517,2662122,38358827,38361515,221996154],"pageRank":0.0012295655325516826}
