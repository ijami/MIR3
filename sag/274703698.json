{"id":274703698,"name":"Additive Approximation Algorithms for Modularity Maximization","abstraction":"The Johnson-Lindenstrauss property (JLP) of random matrices has immense applications in computer science ranging from compressed sensing, learning theory, numerical linear algebra, to privacy. This paper explores the properties and applications of a distribution of random matrices. Our distribution satisfies JLP with desirable properties like fast matrix-vector multiplication, bounded sparsity, and optimal subspace embedding. We can sample a random matrix from this distribution using exactly 3n random bits. We show that a random matrix picked from this distribution preserves differential privacy if the input private matrix satisfies certain spectral properties. This improves the run-time of various differentially private algorithms like Blocki et al. (FOCS 2012) and Upadhyay (ASIACRYPT 13). We also show that successive applications in a specific format of a random matrix picked from our distribution also preserve privacy, and, therefore, allows faster private low-rank approximation algorithm of Upadhyay (arXiv 1409.5414). Since our distribution has bounded column sparsity, this also answers an open problem stated in Blocki et al. (FOCS 2012). We also explore the application of our distribution in manifold learning, and give the first differentially private algorithm for manifold learning if the manifold is smooth. Using the results of Baranuik et al. (Constructive Approximation: 28(3)), our result also implies a distribution of random matrices that satisfies the Restricted-Isometry Property of optimal order for small sparsity. We also show that other known distributions of sparse random matrices with the JLP does not preserve differential privacy, thereby answering one of the open problem posed by Blocki et al. (FOCS 2012). Extending on the work of Kane and Nelson (JACM: 61(1)), we also give a unified analysis of some of the known Johnson-Lindenstrauss transform. We also present a self-contained simplified proof of the known inequality on quadratic form of Gaussian variables that we use in all our proofs. This could be of independent interest.","authors":[],"citedInUrls":[],"refrenceUrls":["https://www.researchgate.net/publication/220617492_The_Fast_Johnson-Lindenstrauss_Transform_and_Approximate_Nearest_Neighbors","https://www.researchgate.net/publication/220779134_Fast_Dimension_Reduction_Using_Rademacher_Series_on_Dual_BCH_Codes","https://www.researchgate.net/publication/45920633_An_Almost_Optimal_Unrestricted_Fast_Johnson-Lindenstrauss_Transform","https://www.researchgate.net/publication/234059911_Fast_and_RIP-optimal_transforms","https://www.researchgate.net/publication/263773706_The_Restricted_Isometry_Property_for_the_General_p-Norms","https://www.researchgate.net/publication/226733217_An_algorithmic_theory_of_learning_Robust_concepts_and_random_projection","https://www.researchgate.net/publication/220343708_Kernels_as_features_On_kernels_margins_and_low-dimensional_mappings","https://www.researchgate.net/publication/224513791_A_Simple_Proof_of_the_Restricted_Isometry_Property_for_Random_Matrices","https://www.researchgate.net/publication/225104615_Random_Projections_of_Smooth_Manifolds","https://www.researchgate.net/publication/2466340_Graph_Approximations_to_Geodesics_on_Embedded_Manifolds"],"citedInIDs":[],"refrenceIDs":[220617492,220779134,45920633,234059911,263773706,226733217,220343708,224513791,225104615,2466340],"pageRank":5.314734299140724E-4}
