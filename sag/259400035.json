{"id":259400035,"name":"Generating Sentences from a Continuous Space","abstraction":"Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and in case of large datasets? We introduce a novel learning and approximate inference method that works efficiently, under some mild conditions, even in the on-line and intractable case. The method involves optimization of a stochastic objective function that can be straightforwardly optimized w.r.t. all parameters, using standard gradient-based optimization methods. The method does not require the typically expensive sampling loops per datapoint required for Monte Carlo EM, and all parameter updates correspond to optimization of the variational lower bound of the marginal likelihood, unlike the wake-sleep algorithm. These theoretical advantages are reflected in experimental results.","authors":["Diederik P Kingma","Max Welling"],"citedInUrls":["https://www.researchgate.net/publication/284219020_Generating_Sentences_from_a_Continuous_Space","https://www.researchgate.net/publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","https://www.researchgate.net/publication/284219537_Adversarial_Autoencoders"],"refrenceUrls":["https://www.researchgate.net/publication/262991675_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","https://www.researchgate.net/publication/234817582_Sample-based_Non-uniform_random_variate_generation"],"citedInIDs":[284219020,284476380,284219537],"refrenceIDs":[262991675,234817582],"pageRank":8.798289135565149E-4}
