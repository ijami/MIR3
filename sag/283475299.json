{"id":283475299,"name":"Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks","abstraction":"Deep learning presents notorious computational challenges. These challenges in- clude, but are not limited to, the non-convexity of learning objectives and estimat- ing the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that exploits the so far unused â€œgeometryâ€? in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with precon- ditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA- grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theo- retically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.","authors":["Edo Collins","Ya-Ping Hsieh","Lawrence Carin","Volkan Cevher"],"citedInUrls":["https://www.researchgate.net/publication/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks"],"refrenceUrls":["https://www.researchgate.net/publication/285779762_Stochastic_Spectral_Descent_for_Discrete_Graphical_Models","https://www.researchgate.net/publication/233410055_Enhanced_Gradient_for_Training_Restricted_Boltzmann_Machines","https://www.researchgate.net/publication/269040844_The_Loss_Surface_of_Multilayer_Networks","https://www.researchgate.net/publication/272423025_RMSProp_and_equilibrated_adaptive_learning_rates_for_non-convex_optimization","https://www.researchgate.net/publication/263011979_Identifying_and_attacking_the_saddle_point_problem_in_high-dimensional_non-convex_optimization","https://www.researchgate.net/publication/220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","https://www.researchgate.net/publication/220319875_Why_Does_Unsupervised_Pre-training_Help_Deep_Learning","https://www.researchgate.net/publication/220116484_Finding_Structure_with_Randomness_Probabilistic_Algorithms_for_Constructing_Approximate_Matrix_Decompositions","https://www.researchgate.net/publication/272746011_A_Practical_Guide_to_Training_Restricted_Boltzmann_Machines_Version_1","https://www.researchgate.net/publication/7017915_A_Fast_Learning_Algorithm_for_Deep_Belief_Nets"],"citedInIDs":[288059869],"refrenceIDs":[285779762,233410055,269040844,272423025,263011979,220320677,220319875,220116484,272746011,7017915],"pageRank":5.740165537111691E-4}
