{"id":226069890,"name":"Cognitive networked sensing and big data","abstraction":"A theorem by Wilks asserts that in smooth parametric density estimation the difference between the maximum likelihood and the likelihood of the sampling distribution converges toward a Chi-square distribution where the number of degrees of freedom coincides with the model dimension. This observation is at the core of some goodness-of-fit testing procedures and of some classical model selection methods. This paper describes a non-asymptotic version of the Wilks phenomenon in bounded contrast optimization procedures. Using concentration inequalities for general functions of independent random variables, it proves that in bounded contrast minimization (as for example in Statistical Learning Theory), the difference between the empirical risk of the minimizer of the true risk in the model and the minimum of the empirical risk (the excess empirical risk) satisfies a Bernstein-like inequality where the variance term reflects the dimension of the model and the scale term reflects the noise conditions. From a mathematical statistics viewpoint, the significance of this result comes from the recent observation that when using model selection via penalization, the excess empirical risk represents a minimum penalty if non-asymptotic guarantees concerning prediction error are to be provided. From the perspective of empirical process theory, this paper describes a concentration inequality for the supremum of a bounded non-centered (actually non-positive) empirical process. Combining the now classical analysis of M-estimation (building on Talagrand’s inequality for suprema of empirical processes) and versatile moment inequalities for functions of independent random variables, this paper develops a genuine Bernstein-like inequality that seems beyond the reach of traditional tools. KeywordsWilks phenomenon–Risk estimates–Suprema of empirical processes–Concentration inequalities–Statistical learning","authors":[],"citedInUrls":["https://www.researchgate.net/publication/267434326_Cognitive_networked_sensing_and_big_data","https://www.researchgate.net/publication/263201089_Critical_dimension_in_profile_semiparametric_estimation","https://www.researchgate.net/publication/257913938_Sharp_deviation_bounds_for_quadratic_forms"],"refrenceUrls":["https://www.researchgate.net/publication/265366875_Local_Rademacher_complexities_and_oracle_inequalities_in_risk_minimization_2004_IMS_Medallion_Lecture_With_discussions_and_rejoinder","https://www.researchgate.net/publication/44241055_EMPIRICAL_PROCESS_THEORY_AND_APPLICATIONS","https://www.researchgate.net/publication/41781172_Concentration_Inequalities_for_Sub-Additive_Functions_Using_the_Entropy_Method","https://www.researchgate.net/publication/38351334_About_the_constants_in_talagrand%27s_concentration_inequalities_for_empirical_processes_Ann_Probab_282863-884_English_summary","https://www.researchgate.net/publication/2397693_A_Bennett_Concentration_Inequality_and_Its_Application_to_Suprema_of_Empirical_Processes","https://www.researchgate.net/publication/245148222_About_the_constants_in_talagrand%27%27s_concentration_inequality","https://www.researchgate.net/publication/236623715_Bias_and_Confidence_in_Not_Quite_Large_Samples","https://www.researchgate.net/publication/230663954_Weak_Convergence_and_Empirical_Process","https://www.researchgate.net/publication/12875093_Algorithmic_Stability_and_Sanity-Check_Bounds_for_Leave-One-Out_Cross-Validation","https://www.researchgate.net/publication/5595969_The_Nature_Of_Statistical_Learning_Theory"],"citedInIDs":[267434326,263201089,257913938],"refrenceIDs":[265366875,44241055,41781172,38351334,2397693,245148222,236623715,230663954,12875093,5595969],"pageRank":5.314734299140724E-4}
