{"id":272846036,"name":"A Survey of Algorithms for Contiguity-Constrained Clustering and Related Problems","abstraction":"Consider a two-class clustering problem where we have $X_i \u003d \\ell_i \\mu + Z_i$, $Z_i \\stackrel{iid}{\\sim} N(0, I_p)$, $1 \\leq i \\leq n$. The class labels $\\ell_i$ are unknown and the main interest is to estimate them. The feature vector $\\mu$ is also unknown but is presumably sparse in that only a small fraction of the features are useful for clustering. We are interested in the fundamental limits. In the two-dimensional phase space calibrating the rarity of the useful features and their strengths, we identify the separating boundary for Region of Impossibility and Region of Possibility. In the former, the useful features are too rare/weak to allow successful clustering. In the latter, the useful features are strong enough and successful clustering is possible. We propose both classical PCA and Important Features PCA (IF-PCA) for clustering. We also propose two aggregation methods for clustering. For any parameter in the Region of Possibility, one or more of these four methods yield successful clustering. We extend the fundamental limits of clustering successfully to many cases where the noise is colored, using Le Cam\u0027s idea on comparison of experiments. We also extend the study to two closely related problems: the signal recovery problem where the interest is to recover the support of $\\mu$, and the hypothesis testing problem where we test whether $X_i$ are samples from $N(0, I_p)$ or are generated according to the model above. We compare the fundamental limits for all three problems and expose interesting insight. We also find an interesting phase transition for IF-PCA. Our results require delicate analysis, especially on post-selection Random Matrix Theory and on lower bound arguments.","authors":[],"citedInUrls":[],"refrenceUrls":["https://www.researchgate.net/publication/1916886_High-dimensional_analysis_of_semidefinite_relaxations_for_sparse_principal_components","https://www.researchgate.net/publication/262950595_Detection_and_Feature_Selection_in_Sparse_Mixture_Models","https://www.researchgate.net/publication/220778887_K-Means_The_Advantages_of_Careful_Seeding","https://www.researchgate.net/publication/237082539_Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation","https://www.researchgate.net/publication/221995234_Controlling_The_False_Discovery_Rate_-_A_Practical_And_Powerful_Approach_To_Multiple_Testing","https://www.researchgate.net/publication/221666048_Optimal_detection_of_sparse_principal_components_in_high_dimension","https://www.researchgate.net/publication/236737613_Optimal_Estimation_and_Rank_Detection_for_Sparse_Spiked_Covariance_Matrices","https://www.researchgate.net/publication/235359914_Sharp_RIP_Bound_for_Sparse_Signal_and_Low-Rank_Matrix_Recovery","https://www.researchgate.net/publication/224224723_Tight_Oracle_Inequalities_for_Low-Rank_Matrix_Recovery_From_a_Minimal_Number_of_Noisy_Random_Measurements","https://www.researchgate.net/publication/8248575_Boosting_for_Tumor_Classification_with_Gene_Expression_Data"],"citedInIDs":[],"refrenceIDs":[1916886,262950595,220778887,237082539,221995234,221666048,236737613,235359914,224224723,8248575],"pageRank":5.314734299140724E-4}
