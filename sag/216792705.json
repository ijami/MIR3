{"id":216792705,"name":"Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency","abstraction":"In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u003e 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).","authors":[],"citedInUrls":["https://www.researchgate.net/publication/288713513_Feedforward_Sequential_Memory_Networks_A_New_Structure_to_Learn_Long-term_Dependency","https://www.researchgate.net/publication/284218796_Density_Modeling_of_Images_using_a_Generalized_Normalization_Transformation","https://www.researchgate.net/publication/284219622_Net2Net_Accelerating_Learning_via_Knowledge_Transfer"],"refrenceUrls":["https://www.researchgate.net/publication/2985446_Gradient-based_learning_applied_to_document_recognition_Proc_IEEE","https://www.researchgate.net/publication/6912170_Reducing_the_Dimensionality_of_Data_with_Neural_Networks","https://www.researchgate.net/publication/13803865_Olshausen_BA_Field_DJ_Sparse_coding_with_an_overcomplete_basis_set_a_strategy_employed_by_V1_Vision_Res_37_3311-3325","https://www.researchgate.net/publication/221345031_Semi-supervised_learning_of_compact_document_representations_with_deep_networks","https://www.researchgate.net/publication/4082304_Learning_methods_for_generic_object_recognition_with_invariance_to_pose_and_lighting","https://www.researchgate.net/publication/200744514_Greedy_layer-wise_training_of_deep_networks","https://www.researchgate.net/publication/221304868_Training_Hierarchical_Feed-Forward_Visual_Recognition_Models_Using_Transfer_Learning_from_Pseudo-Tasks","https://www.researchgate.net/publication/221363014_Discriminative_learned_dictionaries_for_local_image_analysis","https://www.researchgate.net/publication/221620343_Sparse_deep_belief_net_model_for_visual_area_V2","https://www.researchgate.net/publication/4246106_Large-scale_Learning_with_SVM_and_Convolutional_for_Generic_Object_Categorization"],"citedInIDs":[288713513,284218796,284219622],"refrenceIDs":[2985446,6912170,13803865,221345031,4082304,200744514,221304868,221363014,221620343,4246106],"pageRank":0.0012638025387214315}
